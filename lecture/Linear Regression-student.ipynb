{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Previously, we covered: \n",
    "- Computational programming with Python+NumPy, \n",
    "- Visualization with matplotlib, and \n",
    "- Machine learning with SciKit \n",
    "    - Feature Engineering, Model Validation, Supervised Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Digging Deeper: Linear Regression\n",
    "\n",
    "https://flic.kr/p/4uedXX\n",
    "\n",
    "Objectives\n",
    "- Cost functions\n",
    "- Hypothesis functions\n",
    "- Implement our LR algoirthm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Input\n",
    "Generally a **structured** array of real-numbers:\n",
    "- $ x \\in {\\rm I\\!R}^m$\n",
    "\n",
    "### Data Output \n",
    "Vector of real numbers:\n",
    "- $ y \\in {\\rm I\\!R}^1$\n",
    "\n",
    "$x$ and $y$ should both have the same length: $n$. <br>\n",
    "$x$ might have multiple columns representing multiple features: $m$.\n",
    "\n",
    "For instance, for the iris dataset, we had 4 features and 150 data points: $m=4$ and $n=150$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective\n",
    "We are trying to find $y = h(x)$ where $y_i \\approx h(x_i)$ where $i$ is a training example. \n",
    "\n",
    "$h(x)$ is called the **hypothesis function**. \n",
    "\n",
    "Different ML algos have different **$h$ models**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression output\n",
    "$h(x) = \\sum_{i=0}^m w_i x_i + b$\n",
    "\n",
    "This yields a **line** that best fits our data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the built-in models and then build our own. \n",
    "\n",
    "Dataset: Boston housing prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "#Boston housing prices\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#info\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Format: Dictionary \n",
    "print(boston.keys())\n",
    "#Size it up\n",
    "print(boston.data.shape)\n",
    "#What are the 13 features?\n",
    "print(boston.feature_names)\n",
    "\n",
    "boston.target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "Xh = X\n",
    "yh = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of SciKit API\n",
    "\n",
    "1. Choose a class of model by **importing the appropriate estimator** class from Scikit-Learn.\n",
    "2. Choose model **hyperparameters** by instantiating this class with desired values.\n",
    "3. Arrange data into a **features matrix and target vector** following the discussion above.\n",
    "4. **Fit the model** to your data by calling the ``fit()`` method of the model instance.\n",
    "5. **Apply the Model** to new data:\n",
    "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
    "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method.\n",
    "   \n",
    "   \n",
    "``` python\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=0)\n",
    "\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                random_state=42)\n",
    "\n",
    "print(Xtrain.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "### SciKit Implementation\n",
    "\n",
    "Use the sklearn LinearRegression() to:\n",
    "- fit a model on train data\n",
    "- test on model\n",
    "\n",
    "Code below to\n",
    "- measure performance (MSE)\n",
    "- plot true vs predicted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate error and plot\n",
    "mse = mean_squared_error(ytest, y_model)\n",
    "print(\"MSE: \", mse)\n",
    "\n",
    "#compare labels\n",
    "plt.scatter(ytest, y_model)\n",
    "plt.xlabel(\"Truth\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Boston Housing True vs Predicted prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#To see the solution, load or open the following file:\n",
    "# %load ML_lr_1.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model:\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "print(\"Model slope:    \", model.coef_)\n",
    "\n",
    "#y = wx + b\n",
    "#slope: a, intercept: b \n",
    "print(boston.feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But how to find $w$ and $b$? \n",
    "\n",
    "We need the find the set of params that will **minimize the error between the line and the output**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First, we quantify the error\n",
    "\n",
    "We need to quantify the difference between our **linear estimate, $h(x)$, and the labels, $f(x)$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this algorithm, we will use squared error:\n",
    "\n",
    "Calculate the **squared difference between the expected output f(x) and the estimate h(x)**:\n",
    "\n",
    "$E_l = (f(x) - h(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "or for single dimensional case, per-element:\n",
    "\n",
    "$E_l = \\sum_{i=0}^n (y_i - (wx_i + b))^2$\n",
    "<br>  \n",
    "$E_l = \\sum_{i=0}^n (y_i - wx_i - b)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we minimize the error?\n",
    "What is our favorite operation from Calculus that will allow us to find a minima? \n",
    "\n",
    "We set __________ of $E_l$ to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost function\n",
    "\n",
    "Most of the learning algorithms are differ by their **cost or hypothesis function design**. \n",
    "\n",
    "If we look at their cost funcitons, we can understand what they are trying to achieve and how. \n",
    "\n",
    "They may also be called objective functions or loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cost functions are generally represented via **$J(.)$**. \n",
    "\n",
    "$J$ represents the cost function and $.$ represents the parameter that we will we **search over among the hypotheses to minimize the error**. \n",
    "\n",
    "For our case, $E_l$ is our cost function and it represents the amount of error generated by our current prediction of y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing our cost function\n",
    "\n",
    "1D case (single feature): \n",
    "\n",
    "Since we have a cost function and we know how to min a function:\n",
    "\n",
    "$E_l = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "How can we differentiate $E_l(w,b)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need the **chain rule**. \n",
    "\n",
    "Examples from Khan academy:\n",
    "https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review\n",
    "\n",
    "Note: Multivarate representation of this rule from Calc 2 is more appripriate for this class. But, I am keeping it CS Freshman friendly.\n",
    "\n",
    "$\\frac{d}{dx} (f(g(x)) = f'\\big(g(x)\\big) g'(x) $\n",
    "\n",
    "Ex1: $(5-6x)^5$\n",
    "\n",
    "Ex2: $\\sin(2x^3-4x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice\n",
    "Compute the partial derivative of $J$ wrt to $w$:\n",
    "\n",
    "$\\frac{d}{dw} (y - wx)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "$\\frac{d}{dw} (y - wx)^2 = -2x(y - wx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we have two sets of params to optimize, we need to set the partial derivative of $J$ to $0$ for both variables:\n",
    "\n",
    "$J(w,b) = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{dJ}{dw} &= \\sum_{i=0}^n -2x_i(y_i-wx_i-b) \\\\\n",
    "\\frac{dJ}{db} &= \\sum_{i=0}^n -2(y_i-wx_i-b) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then set each equation to 0 to find their min:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\begin{align}\n",
    "\\frac{dJ}{dw} &= \\sum_{i=0}^n -2x_i(y_i-wx_i-b) \\\\\n",
    "&= -\\sum_{i=0}^n x_iy_i  + w\\sum_{i=0}^n x^2_i + b\\sum_{i=0}^n x_i\\\\\n",
    " &= 0 \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\begin{align}\n",
    "\\frac{dJ}{db} &= \\sum_{i=0}^n -2(y_i-w x_i-b) \\\\\n",
    "&= -\\sum_{i=0}^n y_i  + w\\sum_{i=0}^n x_i + b n\\\\\n",
    " &= 0 \\\\\n",
    "\\end{align}$\n",
    "\n",
    "This is a set of linear equations. We can solve them for $w, b$ to minimize the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution to single feature linear regression\n",
    "\n",
    "We have **2 unkowns** and **2 equations**. \n",
    "\n",
    "Closed form solution for the above equations are:\n",
    "\n",
    "$\n",
    "w = \\frac{\\sum_{i=0}^n (x_i - x_m) (y_i - y_m)}{\\sum_{i=0}^n (x_i - x_m)^2} $\n",
    "\n",
    "$b = y_m - wx_m$\n",
    "\n",
    "where $x_m$ and $y_m$ are the means of the $x$ and $y$ vectors, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "Can you rewrite the equation for $w$ using statistical terms?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Implement the single-feature linear regresion to fit the best fit for $xl, yl$ as defined below. \n",
    "\n",
    "Do not estimate your error, but plot the resulting line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Example 1D dataset\n",
    "rng = np.random.RandomState(42) #seed\n",
    "xl = 10 * rng.rand(50) # x with noise\n",
    "yl = 2 * xl - 5 + rng.randn(50) # y is a linear func of x + noise\n",
    "plt.scatter(xl, yl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Solve for w and b using the derivation above \n",
    "# and plot the result of a linear fit\n",
    "# hint: take advantage of np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "print(\"Slope: \",w,b)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "# Plot also the training points\n",
    "plt.scatter(xl, yl, edgecolor='k', s=80)\n",
    "\n",
    "Xt = np.arange(np.min(xl), np.max(xl), 1)\n",
    "yt = (w * Xt) + b\n",
    "plt.plot(Xt, yt, c='r')\n",
    "\n",
    "plt.xlabel(\"xl\")\n",
    "plt.ylabel(\"yl\")\n",
    "plt.title(\"Data and linear fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the solution, load or open the following file:\n",
    "# %load ML_lr_2.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General solution for linear regression\n",
    "\n",
    "If $x$ has multiple features, we can generalize our solution and take advantage of the matrix representation:\n",
    "\n",
    "$J = \\frac{1}{n}\\left\\Vert Xw - y  \\right\\Vert^2 $\n",
    "\n",
    "where $X$ is n by m and $y$ is n by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We take the partial derivative of $J$ wrt to $w$:\n",
    "\n",
    "$J = \\frac{1}{n}\\left\\Vert Xw - y  \\right\\Vert^2 $\n",
    "\n",
    "$\\nabla J(w) = \\frac{2}{n}X^T(Xw - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then set it to 0 \n",
    "\n",
    "$\\nabla J(w) = \\frac{2}{n}X^T(Xw - y) = 0 $ \n",
    "\n",
    "and drop the constants:\n",
    "\n",
    "$X^T Xw = X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember your linear algebra and solve for w:\n",
    "\n",
    "$w = X^py$\n",
    "\n",
    "where $X^p$ is the pseudo-inverse and given that each column of features is linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Implement the linear regression as solved for above to find the coefficients to the housing problem. \n",
    "\n",
    "Predict on test data. \n",
    "\n",
    "Does your error match the scikit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate w for the Boston housing prices using the general solution for linear regression provided above\n",
    "#Predict on test data and plot the true vs predicted prices  \n",
    "\n",
    "# Hint: np.linalg.pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_model.shape, Xtest.shape, w.shape)\n",
    "mse = mean_squared_error(ytest, y_model)\n",
    "print(\"*** MSE: \", mse)\n",
    "\n",
    "#compare labels\n",
    "plt.scatter(ytest, y_model)\n",
    "plt.xlabel(\"Truth\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Boston Housing True vs Predicted prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#To see the solution, load or open the following file:\n",
    "# %load ML_lr_3.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "We set fit_intercept to false for the multi-feature implementation. \n",
    "\n",
    "How can we include it in our implementation? \n",
    "\n",
    "### Hint\n",
    "\n",
    "Our hypothesis function is\n",
    "\n",
    "$E_l = \\sum_{i=1}^n (y_i - (wx_i + b))^2$\n",
    "\n",
    "or\n",
    "\n",
    "$\\begin{align}\n",
    "h(x) &= \\sum_{i=1}^n wx_i + b \\\\\n",
    "&=  w_1x_1 + w_2x_2 + \\ldots + w_mx_m + b\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we classify? \n",
    "\n",
    "We have done regression. Could this algorithm be modified for classificaiton? \n",
    "\n",
    "### Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "We have discussed:\n",
    "\n",
    "- cost function design\n",
    "- hypothesis function for linear regression\n",
    "- using partial derivatives to min the cost function\n",
    "- linear regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "Most ML/AI algorithms rely on computing the cost function, $J(u)$ and its gradient, $\\nabla_uJ(u)$ for any possible $u$.\n",
    "\n",
    " -  the gradient points us in the direction of the steepest increase. \n",
    "\n",
    "Next, we will discuss how we can use **gradient descent algorithm to minimize differentiable cost funtions**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
