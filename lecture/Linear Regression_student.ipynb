{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"PDSH-cover-small.png\">\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Course topics include\n",
    "- computational programming with Python+NumPy, \n",
    "- visualization with matplotlib, and \n",
    "- machine learning with SciKit\n",
    "    - feature engineering, model validation, supervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have covered SciKit API useage for: \n",
    "- supervised/unspurvised learning \n",
    "- model bias and variation (over/under fitting)  \n",
    "- test/train data split  \n",
    "- cross-validaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Digging Deeper: Linear Regression\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Hypothesis function\n",
    "- Cost function\n",
    "- Implement a linear regression algoirthm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Input\n",
    "Generally a **structured** array of real-numbers:\n",
    "- $ x \\in {\\rm I\\!R}^{n \\times m}$\n",
    "\n",
    "### Data Output \n",
    "Vector of real numbers:\n",
    "- $ y \\in {\\rm I\\!R}^{n \\times 1}$\n",
    "\n",
    "$x$ and $y$ should both have the same length: $n$. <br>\n",
    "$x$ might have multiple columns representing multiple features: $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective\n",
    "We are trying to find $y = h(x)$ where $y_i \\approx h(x_i)$ where $i$ is a training example. \n",
    "\n",
    "$h(x)$ is called the **hypothesis function**. \n",
    "\n",
    "Different ML algorithms have different **$h$ models**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression output\n",
    "$h(x) = \\sum_{i=0}^m w_i x_i + b$\n",
    "\n",
    "This yields a **line** that best fits our data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the built-in models and then build our own. \n",
    "\n",
    "Dataset: Boston housing prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "#Boston housing prices\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#info\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Format: Dictionary \n",
    "print(boston.keys())\n",
    "#Size it up\n",
    "print(boston.data.shape)\n",
    "#What are the 13 features?\n",
    "print(boston.feature_names)\n",
    "\n",
    "boston.target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "Xh = X\n",
    "yh = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review of SciKit API\n",
    "\n",
    "According to VanderPlas (2016):\n",
    "1. Choose a class of model by **importing the appropriate estimator** class from Scikit-Learn.\n",
    "2. Choose model **hyperparameters** by instantiating this class with desired values.\n",
    "3. Arrange data into a **features matrix and target vector** following the discussion above.\n",
    "4. **Fit the model** to your data by calling the ``fit()`` method of the model instance.\n",
    "5. **Apply the model** to new data:\n",
    "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
    "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method. (p. 347)\n",
    "   \n",
    "   \n",
    "``` python\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=0)\n",
    "\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)\n",
    "```\n",
    "\n",
    "VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. Sebastopol, CA: O'Reilly Media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                random_state=42)\n",
    "\n",
    "print(Xtrain.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "### SciKit Implementation\n",
    "\n",
    "Use the sklearn LinearRegression() to:\n",
    "- fit a model on training data\n",
    "- predict using the model\n",
    "\n",
    "Code below to:\n",
    "- measure performance using mean squared error (MSE)\n",
    "- plot true vs predicted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate error and plot\n",
    "mse = mean_squared_error(ytest, y_model)\n",
    "print(\"MSE: \", mse)\n",
    "\n",
    "#compare labels\n",
    "plt.scatter(ytest, y_model)\n",
    "plt.xlabel(\"Truth\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Boston Housing True vs Predicted prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#To see a solution, load or open the following file:\n",
    "%load ML_lr_1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model:\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "print(\"Model slope:    \", model.coef_)\n",
    "\n",
    "#y = wx + b\n",
    "#slope: a, intercept: b \n",
    "print(boston.feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But how to find $w$ and $b$? \n",
    "\n",
    "We need the find the set of parameters that will **minimize the error between the line and the output**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First, we quantify the error\n",
    "\n",
    "We need to quantify the difference between our **linear estimate, $h(x)$, and the labels, $f(x)$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this algorithm, we will use squared error:\n",
    "\n",
    "Calculate the **squared difference between the expected output f(x) and the estimate h(x)**:\n",
    "\n",
    "$E_l = (f(x) - h(x))^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "or for single dimensional case, per-element:\n",
    "\n",
    "$E_l = \\sum_{i=0}^n (y_i - (wx_i + b))^2$\n",
    "<br>  \n",
    "$E_l = \\sum_{i=0}^n (y_i - wx_i - b)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we minimize the error?\n",
    "\n",
    "What is our favorite operation from Calculus that will allow us to find its minima? \n",
    "\n",
    "We set __________ of $E_l$ to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost function\n",
    "\n",
    "Most of the learning algorithms differ by their **cost or hypothesis function design**. \n",
    "\n",
    "If we look at their cost functions, we can understand what they are trying to achieve and how. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cost functions are generally represented via **$J(.)$**. \n",
    "\n",
    "$J$ represents the cost function and $.$ represents the parameter that we will **search over among the hypotheses to minimize the error**. \n",
    "\n",
    "For our case, $E_l$ is our cost function and it represents the amount of error generated by our current prediction of $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing 1D cost function\n",
    "\n",
    "1D case (single feature): \n",
    "\n",
    "$J(w,b) = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "\n",
    "Since we have a cost function and we know how to minimize a function:\n",
    "\n",
    "$E_l = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "How can we differentiate $E_l(w,b)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need the **chain rule**. \n",
    "\n",
    "$\\frac{d}{dx} (f(g(x)) = f'\\big(g(x)\\big) g'(x) $\n",
    "\n",
    "Ex: $(5-6x)^5$\n",
    "\n",
    "If you need a refresher or more examples: [Khan academy](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice\n",
    "Compute the partial derivative of $J$ wrt to $w$:\n",
    "\n",
    "$\\frac{d}{dw} (y - wx)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "*Enter your solution here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we have two sets of parameters to optimize, we need to set the partial derivative of $J$ to $0$ for both variables:\n",
    "\n",
    "$J(w,b) = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{dJ}{dw} &= \\sum_{i=0}^n -2x_i(y_i-wx_i-b) \\\\\n",
    "\\frac{dJ}{db} &= \\sum_{i=0}^n -2(y_i-wx_i-b) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then set each equation to 0 to find their min:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\begin{align}\n",
    "\\frac{dJ}{dw} &= \\sum_{i=0}^n -2x_i(y_i-wx_i-b) \\\\\n",
    "&= -\\sum_{i=0}^n x_iy_i  + w\\sum_{i=0}^n x^2_i + b\\sum_{i=0}^n x_i\\\\\n",
    " &= 0 \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\begin{align}\n",
    "\\frac{dJ}{db} &= \\sum_{i=0}^n -2(y_i-w x_i-b) \\\\\n",
    "&= -\\sum_{i=0}^n y_i  + w\\sum_{i=0}^n x_i + b n\\\\\n",
    " &= 0 \\\\\n",
    "\\end{align}$\n",
    "\n",
    "This is a set of linear equations. We can solve them for $w, b$ to minimize the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution to single feature linear regression\n",
    "\n",
    "We have **2 unkowns** and **2 equations**. \n",
    "\n",
    "Closed form solution for the above equations are:\n",
    "\n",
    "$\n",
    "w = \\frac{\\sum_{i=0}^n (x_i - x_m) (y_i - y_m)}{\\sum_{i=0}^n (x_i - x_m)^2} $\n",
    "\n",
    "$b = y_m - wx_m$\n",
    "\n",
    "where $x_m$ and $y_m$ are the means of the $x$ and $y$ vectors, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion \n",
    "\n",
    "Can you rewrite the equation for $w$ using statistical terms?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Implement the single-feature linear regresion to find the best fit for $xl, yl$ as defined below. \n",
    "\n",
    "Plot the resulting line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Example 1D dataset\n",
    "rng = np.random.RandomState(42) #seed\n",
    "xl = 10 * rng.rand(50) # x with noise\n",
    "yl = 2 * xl - 5 + rng.randn(50) # y is a linear func of x + noise\n",
    "plt.scatter(xl, yl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Solve for w and b using the derivation above \n",
    "# and plot the result of a linear fit\n",
    "# hint: take advantage of np.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "print(\"Slope: \",w,b)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "# Plot also the training points\n",
    "plt.scatter(xl, yl, edgecolor='k', s=80)\n",
    "\n",
    "Xt = np.arange(np.min(xl), np.max(xl), 1)\n",
    "yt = (w * Xt) + b\n",
    "plt.plot(Xt, yt, c='r')\n",
    "\n",
    "plt.xlabel(\"xl\")\n",
    "plt.ylabel(\"yl\")\n",
    "plt.title(\"Data and linear fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see a solution, load or open the following file:\n",
    "%load ML_lr_2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General solution for linear regression\n",
    "\n",
    "If $x$ has multiple features, we can generalize our solution and take advantage of the matrix representation:\n",
    "\n",
    "$J = \\frac{1}{n}\\left\\Vert Xw - y  \\right\\Vert^2 $\n",
    "\n",
    "where $X$ is n by m and $y$ is n by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We take the partial derivative of $J$ wrt to $w$:\n",
    "\n",
    "$J = \\frac{1}{n}\\left\\Vert Xw - y  \\right\\Vert^2 $\n",
    "\n",
    "$\\nabla J(w) = \\frac{2}{n}X^T(Xw - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then set it to 0 \n",
    "\n",
    "$\\nabla J(w) = \\frac{2}{n}X^T(Xw - y) = 0 $ \n",
    "\n",
    "and drop the constants:\n",
    "\n",
    "$X^T Xw = X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember your linear algebra and solve for $w$:\n",
    "\n",
    "$\\begin{align}\n",
    "w &= (X^T X)^{-1}X^Ty \\\\\n",
    "&= X^+y \n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "where $X^+$ is the pseudo-inverse and given that each column of features is linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Implement the linear regression as derived above to find the coefficients to solve the housing problem. \n",
    "\n",
    "Predict on test data, Xtest. \n",
    "\n",
    "Do your error/coefficients match the scikit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Calculate w for the Boston housing prices using the general solution for linear regression provided above\n",
    "#2. Predict on test data and plot the true vs predicted prices  \n",
    "\n",
    "# Variables: Xtrain, Xtest, ytrain, ytest\n",
    "# Hint: np.linalg.pinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_model.shape, Xtest.shape, w.shape)\n",
    "mse = mean_squared_error(ytest, y_model)\n",
    "print(\"*** MSE: \", mse)\n",
    "\n",
    "#compare labels\n",
    "plt.scatter(ytest, y_model)\n",
    "plt.xlabel(\"Truth\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Boston Housing True vs Predicted prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#To see a solution, load or open the following file:\n",
    "%load ML_lr_3.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion: Intercept\n",
    "\n",
    "We set fit_intercept to false for the multi-feature implementation. \n",
    "\n",
    "How can we include it in our calculations? \n",
    "\n",
    "### Hint\n",
    "\n",
    "Our cost function is\n",
    "\n",
    "$J(w,b) = \\sum_{i=0}^n (y_i - wx_i - b)^2$\n",
    "\n",
    "where\n",
    "\n",
    "$\\begin{align}\n",
    "h(x) &= \\sum_{i=1}^n wx_i + b \\\\\n",
    "&=  b+w_1x_1 + w_2x_2 + \\ldots + w_mx_m \n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "We have discussed:\n",
    "\n",
    "- hypothesis function for linear regression: $h(x) = \\sum_{i=0}^m w_i x_i + b$\n",
    "- cost function design, $J(w)= \\frac{1}{n}\\left\\Vert Xw - y  \\right\\Vert^2 $\n",
    "    - minimize the cost function using partial derivatives\n",
    "- linear regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "Most ML/AI algorithms rely on computing the cost function, $J(u)$ and its gradient, $\\nabla_uJ(u)$ for any possible $u$.\n",
    "\n",
    " -  the gradient points us in the direction of the steepest increase. \n",
    "\n",
    "Next, we will discuss how we can use **gradient descent algorithm to minimize differentiable cost funtions**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
